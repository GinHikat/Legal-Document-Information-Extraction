{"cells":[{"cell_type":"code","execution_count":null,"id":"861630a7","metadata":{"id":"861630a7"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from torchcrf import CRF\n","\n","class MaskedAttention(nn.Module):\n","    def __init__(self, hidden_dim):\n","        super().__init__()\n","        self.attn = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, lstm_out, mask):\n","        # lstm_out: (batch, seq_len, hidden_dim)\n","        scores = self.attn(lstm_out).squeeze(-1)\n","        scores = scores.masked_fill(mask == 0, -1e9)\n","        attn_weights = torch.softmax(scores, dim=1).unsqueeze(-1)\n","        context = torch.sum(lstm_out * attn_weights, dim=1, keepdim=True)\n","        return lstm_out + context.expand_as(lstm_out)\n","\n","\n","class BiLSTM_CRF(nn.Module):\n","    def __init__(self, vocab_size, tagset_size, embedding_dim=128, hidden_dim=256, num_layers=2, dropout=0.25, pad_idx=0):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n","        self.embedding_dropout = nn.Dropout(dropout)\n","\n","        self.bilstm = nn.LSTM(\n","            embedding_dim,\n","            hidden_dim // 2,\n","            num_layers=num_layers,\n","            bidirectional=True,\n","            batch_first=True,\n","            dropout=dropout if num_layers > 1 else 0\n","        )\n","\n","        self.layer_norm = nn.LayerNorm(hidden_dim)\n","        self.attention = MaskedAttention(hidden_dim)\n","\n","        self.hidden_fc = nn.Sequential(\n","            nn.Linear(hidden_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(dropout)\n","        )\n","\n","        self.fc = nn.Linear(hidden_dim, tagset_size)\n","        self.crf = CRF(tagset_size, batch_first=True)\n","\n","    def forward(self, x, tags=None, mask=None):\n","        if mask is None:\n","            mask = (x != self.embedding.padding_idx).type(torch.bool)\n","        mask[:, 0] = 1\n","\n","        embeddings = self.embedding(x)\n","        embeddings = self.embedding_dropout(embeddings)\n","\n","        lstm_out, _ = self.bilstm(embeddings)\n","        lstm_out = self.layer_norm(lstm_out)\n","        lstm_out = self.attention(lstm_out, mask)\n","        lstm_out = self.hidden_fc(lstm_out)\n","\n","        emissions = self.fc(lstm_out)\n","\n","        if tags is not None:\n","            log_likelihood = self.crf(emissions, tags, mask=mask)\n","            return -log_likelihood.mean()\n","        else:\n","            return self.crf.decode(emissions, mask=mask)"]},{"cell_type":"code","execution_count":null,"id":"114e46d0","metadata":{"id":"114e46d0","outputId":"1e172a1c-7a6f-4882-ff2c-2acfbcaedf80"},"outputs":[{"name":"stdout","output_type":"stream","text":["✅ Model and dictionaries loaded successfully!\n"]}],"source":["## Test NER model\n","import torch\n","import json\n","from nltk import sent_tokenize\n","import unicodedata\n","\n","with open(\"D:/Study/Education/Projects/Group_Project/rag_model/model/NER/token2idx.json\", \"r\", encoding=\"utf-8\") as f:\n","    token2idx = json.load(f)\n","\n","with open(\"D:/Study/Education/Projects/Group_Project/rag_model/model/NER/label2idx.json\", \"r\", encoding=\"utf-8\") as f:\n","    label2idx = json.load(f)\n","\n","#Reverse lookup dict\n","idx2label = {v: k for k, v in label2idx.items()}\n","\n","model = BiLSTM_CRF(\n","    vocab_size=len(token2idx),\n","    tagset_size=len(label2idx),\n","    embedding_dim=128,\n","    hidden_dim=256,\n","    num_layers=2,\n","    dropout=0.3,\n","    pad_idx=token2idx[\"<PAD>\"]\n",")\n","\n","model.load_state_dict(torch.load(\"D:/Study/Education/Projects/Group_Project/rag_model/model/NER/model_bilstm_crf.pt\", map_location=torch.device(\"cpu\")))\n","model.eval()\n","\n","print(\"✅ Model and dictionaries loaded successfully!\")"]},{"cell_type":"code","execution_count":null,"id":"053d7042","metadata":{"id":"053d7042","outputId":"c849d2f9-9056-4607-b38c-79c501acb8d6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cpu\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"Using device:\", device)"]},{"cell_type":"code","execution_count":null,"id":"76c1efbb","metadata":{"id":"76c1efbb","outputId":"6ed86b29-a76a-46a3-c169-2dd978ded4a0"},"outputs":[{"data":{"text/plain":["[('luật', 'B-TIT'),\n"," ('thuế', 'I-TIT'),\n"," ('giá', 'I-TIT'),\n"," ('trị', 'I-TIT'),\n"," ('gia', 'I-TIT'),\n"," ('tăng', 'I-TIT'),\n"," ('số', 'O'),\n"," ('12', 'B-DOCID'),\n"," ('/', 'I-DOCID'),\n"," ('23', 'I-DOCID'),\n"," ('/', 'I-DOCID'),\n"," ('nđ', 'I-DOCID'),\n"," ('-', 'O'),\n"," ('cp', 'O'),\n"," ('được', 'O'),\n"," ('ban', 'O'),\n"," ('hành', 'O'),\n"," ('bởi', 'O'),\n"," ('chính', 'B-DEP'),\n"," ('phủ', 'I-DEP'),\n"," ('và', 'O'),\n"," ('được', 'O'),\n"," ('kí', 'O'),\n"," ('kết', 'O'),\n"," ('bởi', 'O'),\n"," ('chủ', 'O'),\n"," ('tịch', 'O'),\n"," ('quốc', 'B-LOC'),\n"," ('hội', 'I-LOC'),\n"," ('trần', 'I-LOC'),\n"," ('thanh', 'I-LOC'),\n"," ('mẫn', 'I-LOC'),\n"," ('hà', 'I-LOC'),\n"," ('nội', 'I-LOC'),\n"," ('ngày', 'B-DAT'),\n"," ('23', 'I-DAT'),\n"," ('tháng', 'I-DAT'),\n"," ('8', 'I-DAT'),\n"," ('năm', 'I-DAT'),\n"," ('2021', 'I-DAT')]"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import re\n","\n","def prepare_text_for_model(text, token2idx, device=\"cpu\"):\n","\n","    text = text.lower().strip()\n","    tokens = re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n","\n","    X = [token2idx.get(tok, token2idx[\"<UNK>\"]) for tok in tokens]\n","\n","    X_tensor = torch.tensor([X], dtype=torch.long).to(device)\n","    mask = (X_tensor != token2idx[\"<PAD>\"]).to(torch.bool)\n","\n","    return X_tensor, mask, tokens\n","\n","# text = sent_tokenize(try_thong_tu)[0]\n","text = \"Luật Thuế Giá trị gia tăng số 12/23/Nđ-CP được ban hành bởi Chính phủ và được kí kết bởi Chủ tịch QUốc hội Trần Thanh Mẫn Hà Nội ngày 23 tháng 8 năm 2021\"\n","\n","X_tensor, mask, tokens = prepare_text_for_model(text, token2idx, device)\n","\n","with torch.no_grad():\n","    pred = model(X_tensor, mask=mask)\n","\n","pred_labels = [idx2label[i] for i in pred[0]]\n","list(zip(tokens, pred_labels))"]},{"cell_type":"code","execution_count":null,"id":"e46242db","metadata":{"id":"e46242db"},"outputs":[],"source":["from vncorenlp import VnCoreNLP\n","\n","ner_annotator = VnCoreNLP(\"D:/Study/Education/Projects/Group_Project/VnCoreNLP/VnCoreNLP-1.1.1.jar\", annotators=\"wseg,pos,ner\", max_heap_size='-Xmx2g')"]},{"cell_type":"markdown","id":"bc3264ba","metadata":{"id":"bc3264ba"},"source":["#### Try Data Extraction framework"]},{"cell_type":"code","execution_count":null,"id":"a1969e15","metadata":{"id":"a1969e15"},"outputs":[],"source":["from nltk import sent_tokenize\n","\n","#for module import\n","import sys, os\n","project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../..\"))\n","if project_root not in sys.path:\n","    sys.path.append(project_root)\n","\n","from shared_functions.global_functions import *"]},{"cell_type":"code","execution_count":null,"id":"326eecd9","metadata":{"id":"326eecd9"},"outputs":[],"source":["def tokenize_like_conll(text):\n","    \"\"\"\n","    Split tokens like Label Studio:\n","    \"\"\"\n","    return re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n","\n","def run_ner_on_sentence(sentence, model, token2idx, idx2label, device=\"cpu\"):\n","    \"\"\"\n","    Tokenize a sentence, convert to indices (alreay had indices dictionary), run NER model, and return (tokens, labels).\n","    \"\"\"\n","    tokens = tokenize_like_conll(sentence.lower())\n","\n","    X = [token2idx.get(tok, token2idx[\"<UNK>\"]) for tok in tokens]\n","    X_tensor = torch.tensor([X], dtype=torch.long).to(device)\n","\n","    mask = (X_tensor != token2idx[\"<PAD>\"]).to(torch.bool)\n","\n","    # Predictx label from model\n","    model.eval()\n","    with torch.no_grad():\n","        preds = model(X_tensor, mask=mask)[0]\n","        preds = preds[:len(tokens)]\n","\n","    labels = [idx2label[p] for p in preds]\n","\n","    return tokens, labels\n","\n","\n","def merge_entities(tokens, labels):\n","    \"\"\"\n","    Merge consecutive tokens with the same entity type (B-I-O format).\n","    Returns list of (text, entity_type)\n","    Example: Hà - B-LOC, Nội - I-LOC  => (\"Hà Nội\", \"LOC\")\n","    \"\"\"\n","    entities = []\n","    current_tokens = []\n","    current_type = None\n","\n","    for tok, lbl in zip(tokens, labels):\n","        if lbl.startswith(\"B-\"):\n","            if current_tokens:\n","                entities.append((\" \".join(current_tokens), current_type))\n","            current_tokens = [tok]\n","            current_type = lbl[2:]\n","        elif lbl.startswith(\"I-\") and current_tokens:\n","            current_tokens.append(tok)\n","        else:\n","            if current_tokens:\n","                entities.append((\" \".join(current_tokens), current_type))\n","                current_tokens = []\n","                current_type = None\n","\n","    if current_tokens:\n","        entities.append((\" \".join(current_tokens), current_type))\n","    return entities\n","\n","def normalize_text(text):\n","    if not text:\n","        return None\n","    text = re.sub(r\"\\s+\", \" \", text.strip())\n","    text = text.replace(\"–\", \"-\").replace(\"_\", \" \")\n","    text = re.sub(r\"[-–—,:;.\\s]+$\", \"\", text)\n","    return text\n","\n","def merge_fragmented(text):\n","    \"\"\"\n","    Merge the token starts with a vowel with the token before it\n","    EXCEPT for certain Vietnamese words\n","    \"\"\"\n","    if not isinstance(text, str):\n","        return text\n","\n","    vowels = \"aeiouyàáảãạăắằẳẵặâấầẩẫậèéẻẽẹêếềểễệìíỉĩịòóỏõọôốồổỗộơớờởỡợùúủũụưứừửữựýỳỷỹỵ\"\n","    vowel_start = re.compile(rf\"^[{vowels}]\", re.IGNORECASE)\n","\n","    # Words that should *not* be merged (valid Vietnamese words that varies based on use cases)\n","    exclude_words = {\"án\", 'anh'}\n","\n","    parts = text.split()\n","    if len(parts) < 2:\n","        return text\n","\n","    merged = [parts[0]]\n","    for token in parts[1:]:\n","        token_lower = token.lower()\n","        if vowel_start.match(token_lower) and token_lower not in exclude_words:\n","            merged[-1] += token\n","        else:\n","            merged.append(token)\n","\n","    return \" \".join(merged)\n","\n","def normalize_date(text):\n","    \"\"\"\n","    Convert various Vietnamese date formats to 'dd/mm/yyyy'.\n","    Examples:\n","        'ngày 12 tháng 6 năm 2021'  -> '12/06/2021'\n","        '17/6/2025'                 -> '17/06/2025'\n","        '17/6'                      -> '17/06/'\n","        'ngày 5 tháng 12 năm 24'    -> '05/12/2024'\n","    \"\"\"\n","    if not text:\n","        return None\n","\n","    text = text.strip().lower()\n","    text = text.replace(\"–\", \"-\").replace(\"_\", \" \")\n","\n","    # Try to match \"ngày 12 tháng 6 năm 2021\"\n","    match_vn = re.search(\n","        r\"ngày\\s*(\\d{1,2})\\D+tháng\\s*(\\d{1,2})\\D+năm\\s*(\\d{2,4})\",\n","        text,\n","        re.IGNORECASE\n","    )\n","    if match_vn:\n","        d, m, y = match_vn.groups()\n","        if len(y) == 2:\n","            y = \"20\" + y\n","        return f\"{d.zfill(2)}/{m.zfill(2)}/{y}\"\n","\n","    # Try to match \"17/6/2025\" or \"17-6-25\" or \"17.6.2025\"\n","    match_num = re.search(r\"(\\d{1,2})[^\\d]+(\\d{1,2})(?:[^\\d]+(\\d{2,4}))?\", text)\n","    if match_num:\n","        d, m, y = match_num.groups()\n","        if y:\n","            if len(y) == 2:\n","                y = \"20\" + y\n","            return f\"{d.zfill(2)}/{m.zfill(2)}/{y}\"\n","        else:\n","            return f\"{d.zfill(2)}/{m.zfill(2)}\"\n","\n","\n","    # If no recognizable format found, return cleaned text\n","    return re.sub(r\"\\s+\", \" \", text.strip())\n","\n","def extract_abbreviation(text):\n","    def remove_accents(s):\n","        return ''.join(\n","            c for c in unicodedata.normalize('NFD', s)\n","            if unicodedata.category(c) != 'Mn'\n","            )\n","\n","    parts = remove_accents(text).strip().split()\n","    abbreviation = ''.join(p[0].upper() for p in parts if p)\n","\n","    return abbreviation\n","\n","def clean_document_id(id_str):\n","    if not isinstance(id_str, str):\n","        return id_str\n","    id_str = id_str.lower().strip()\n","    # Common noise patterns\n","    replacements = {\n","        r'cpu\\b': 'cp',\n","        r'qhu\\b': 'qh',\n","        r'bnvu\\b': 'bnv',\n","        r'bkhu\\b': 'bkh',\n","        r'ttu\\b': 'tt',\n","        r'nhnnvn':'nhnn',\n","        r'bkhvcn':'bkhcn'\n","    }\n","    for pattern, repl in replacements.items():\n","        id_str = re.sub(pattern, repl, id_str)\n","    return id_str\n","\n","def to_upper_alnum(text):\n","    '''\n","    Uppercase all words in a string\n","    '''\n","    if not isinstance(text, str):\n","        return text\n","    return re.sub(r'[a-zA-ZÀ-ỹ]', lambda m: m.group(0).upper(), text)"]},{"cell_type":"code","execution_count":null,"id":"bd431770","metadata":{"id":"bd431770"},"outputs":[],"source":["def extract_document_metadata(text, model, token2idx, idx2label, device=\"cpu\"):\n","    sentences = sent_tokenize(text)\n","    if not sentences:\n","        return pd.DataFrame([{}])\n","\n","    first_sentence = sentences[0]\n","    last_sentence = sentences[-1]\n","\n","    tokens, labels = run_ner_on_sentence(first_sentence, model, token2idx, idx2label, device)\n","    entities = merge_entities(tokens, labels)\n","    # entities = smart_fix_loc(entities)\n","\n","    last_tokens, last_labels = run_ner_on_sentence(last_sentence, model, token2idx, idx2label, device)\n","    last_entities = merge_entities(last_tokens, last_labels)\n","\n","    metadata = {\n","        \"issuer_department\": None,\n","        \"issue_date\": None,\n","        \"title\": None,\n","        \"location\": None,\n","        \"document_id\": None,\n","        \"issuer\": None,\n","        \"document_type\": None\n","    }\n","\n","    #Extract entities from first sentence\n","    seen_loc = 0\n","    for text_, etype in entities:\n","        if etype == \"DEP\" and metadata[\"issuer_department\"] is None:\n","            metadata[\"issuer_department\"] = normalize_text(text_)\n","\n","        elif etype == \"DAT\" and metadata[\"issue_date\"] is None:\n","            metadata[\"issue_date\"] = normalize_date(text_)\n","\n","        elif etype == \"LOC\":\n","            seen_loc += 1\n","            if seen_loc == 2 and metadata[\"location\"] is None:\n","                metadata[\"location\"] = normalize_text(text_)\n","\n","        elif etype == \"DOCID\" and metadata[\"document_id\"] is None:\n","            if text_.endswith(\"-\"):\n","                text_ += \"cp\"\n","            elif len(text_.split('/')[-1]) <= 2:\n","                text_ += f'-{extract_abbreviation(metadata[\"issuer_department\"])}'\n","\n","            metadata[\"document_id\"] = re.sub(r\"\\s+\", \"\", normalize_text(text_))\n","\n","    #auto-fill missing year in issue_date by document_id\n","    if metadata.get(\"issue_date\"):\n","        date_str = metadata[\"issue_date\"]\n","        if date_str.count('/') == 1:\n","            date_str = f'{date_str}/{metadata[\"document_id\"].split(\"/\")[1]}'\n","            metadata[\"issue_date\"] = date_str\n","\n","    #default location to Hanoi\n","    if not metadata[\"location\"]:\n","        metadata[\"location\"] = \"Hà Nội\"\n","\n","    title_keywords = [\"luật\", \"nghị quyết\", \"nghị định\", \"thông tư\", \"quyết định\", \"chỉ thị\", \"hướng dẫn\"]\n","    title_finding =['luật', 'nghị', 'thông', 'quyết', 'chỉ', 'hướng']\n","\n","    # Reassign labels for title-start tokens\n","    for i, tok in enumerate(tokens):\n","        if tok.lower() in title_finding:\n","            labels[i] = \"B-TIT\"\n","\n","    first_b_tit_idx = next((i for i, lbl in enumerate(labels) if lbl == \"B-TIT\"), None)\n","\n","    if first_b_tit_idx is not None:\n","        start_idx = first_b_tit_idx\n","        end_idx = start_idx\n","\n","        for k in range(start_idx + 1, len(tokens)):\n","            if labels[k].endswith(\"TIT\"):\n","                end_idx = k\n","            else:\n","                if (end_idx - start_idx + 1) < 5:\n","                    end_idx = min(start_idx + 4, len(tokens) - 1)\n","                break\n","\n","        candidate_title = \" \".join(tokens[start_idx:end_idx + 1])\n","        candidate_title = normalize_text(candidate_title)\n","        metadata[\"title\"] = candidate_title\n","\n","        for kw in title_keywords:\n","            if candidate_title.lower().startswith(kw):\n","                metadata[\"document_type\"] = kw.capitalize()\n","                break\n","    else:\n","        metadata[\"title\"] = \"UNKNOWN\"\n","        metadata[\"document_type\"] = \"UNKNOWN\"\n","\n","    # Second “Luật” occurrence logic\n","    if metadata[\"document_type\"] == \"Luật\":\n","        luat_indices = [i for i, t in enumerate(tokens) if t.lower() == \"luật\"]\n","        if len(luat_indices) >= 2:\n","            start_idx = luat_indices[1]\n","            end_idx = start_idx\n","            for k in range(start_idx + 1, len(tokens)):\n","                if labels[k].endswith(\"TIT\"):\n","                    end_idx = k\n","                else:\n","                    if (end_idx - start_idx + 1) < 5:\n","                        end_idx = min(start_idx + 4, len(tokens) - 1)\n","                    break\n","            metadata[\"title\"] = normalize_text(\" \".join(tokens[start_idx:end_idx + 1]))\n","\n","    #ensure consistency between title and document_type (fill each other)\n","    if metadata[\"title\"] and metadata[\"document_type\"]:\n","        title_lower = metadata[\"title\"].lower()\n","        doc_type_lower = metadata[\"document_type\"].lower()\n","        if not title_lower.startswith(doc_type_lower):\n","            metadata[\"title\"] = f\"{metadata['document_type']} {metadata['title']}\"\n","    elif metadata[\"title\"]:\n","        for kw in title_keywords:\n","            if metadata[\"title\"].lower().startswith(kw):\n","                metadata[\"document_type\"] = kw.capitalize()\n","                break\n","\n","    ## Issuer is extracted using the last complete PER entity\n","    try:\n","        annotated = ner_annotator.annotate(last_sentence)\n","        persons = []\n","        current_name = []\n","\n","        for sent in annotated[\"sentences\"]:\n","            for word_info in sent:\n","                label = word_info.get(\"nerLabel\")\n","                token = word_info.get(\"form\", \"\").replace(\"_\", \" \")\n","                if label == \"B-PER\":\n","                    if current_name:\n","                        persons.append(\" \".join(current_name))\n","                    current_name = [token]\n","                elif label == \"I-PER\" and current_name:\n","                    current_name.append(token)\n","                else:\n","                    if current_name:\n","                        persons.append(\" \".join(current_name))\n","                        current_name = []\n","        if current_name:\n","            persons.append(\" \".join(current_name))\n","\n","        if persons:\n","            metadata[\"issuer\"] = normalize_text(persons[-1])\n","        else:\n","            metadata[\"issuer\"] = \"UNKNOWN\"\n","\n","        if len(metadata[\"issuer\"].split(' ')) == 1:\n","            metadata[\"issuer\"] = \"UNKNOWN\"\n","\n","    except Exception as e:\n","        print(f\"[WARN] VnCoreNLP extraction failed: {e}\")\n","        metadata[\"issuer\"] = \"UNKNOWN\"\n","\n","    df = pd.DataFrame([metadata])\n","\n","    #Fix missing document id\n","    mask = df['document_id'].str.split('/').str[-1].apply(lambda x: len(str(x).strip()) <= 2)\n","\n","    df.loc[mask, 'document_id'] = (\n","        df.loc[mask, 'document_id'] + '-' +\n","        df.loc[mask, 'issuer_department'].map(str.strip).apply(extract_abbreviation).str.lower()\n","    )\n","\n","    df['document_id'] = df['document_id'].apply(clean_document_id)\n","\n","    #Fix some fragmented features\n","    df = df.applymap(merge_fragmented)\n","\n","    cols_to_capitalize = ['issuer_department', 'title', 'location', 'issuer', 'document_type']\n","    df[cols_to_capitalize] = df[cols_to_capitalize].apply(\n","        lambda col: col.apply(lambda x: x.title() if isinstance(x, str) else x)\n","    )\n","\n","    df['document_id'] = df['document_id'].apply(to_upper_alnum)\n","\n","    return df"]},{"cell_type":"code","execution_count":null,"id":"7b87f1fe","metadata":{"id":"7b87f1fe"},"outputs":[],"source":["file_path = 'D:/Study/Education/Projects/Group_Project/source/document'\n","files = os.listdir(file_path)\n","\n","list_doc_content = []\n","for file in files:\n","    if not file.lower().endswith(('.pdf', '.docx', '.doc')):\n","        continue\n","    list_doc_content.append(get_text_from_s3(f'legaldocstorage/{file}'))"]},{"cell_type":"code","execution_count":null,"id":"da874729","metadata":{"id":"da874729","outputId":"c7a1c38a-e5b3-427f-c690-1a270368f5fb"},"outputs":[{"name":"stderr","output_type":"stream","text":["C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n","C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n","C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n","C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n","C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n","C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n","C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n","C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n","C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n","C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n","C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n","C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n","C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n","C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n","C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n","C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n","C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n","C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n","C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n","C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n","C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_9824\\152228799.py:174: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n","  df = df.applymap(merge_fragmented)\n"]}],"source":["# import botocore\n","\n","# for file in files:\n","#     try:\n","#         try_text = get_text_from_s3(f'legaldocstorage/{file}')\n","#         df_meta = extract_document_metadata(try_text, model, token2idx, idx2label, device)\n","#         df = pd.concat([df, df_meta], axis=0)\n","#     except botocore.exceptions.ClientError as e:\n","#         if e.response['Error']['Code'] == 'NoSuchKey':\n","#             print(f\"❌ NoSuchKey error: File not found in S3 → {file}\")\n","#         else:\n","#             print(f\"⚠️ Other S3 error for file {file}: {e}\")\n","#     except Exception as e:\n","#         print(f\"⚠️ Unexpected error with file {file}: {e}\")\n","\n","df = pd.DataFrame(columns = ['issuer_department', 'issue_date', 'title', 'location', 'document_id', 'issuer', 'document_type'])\n","\n","for doc_content in list_doc_content:\n","    df_meta = extract_document_metadata(doc_content, model, token2idx, idx2label, device)\n","    df = pd.concat([df, df_meta], axis=0)\n","df = df.reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"id":"30c6ac10","metadata":{"id":"30c6ac10","outputId":"5d69eb44-f154-4709-d3ae-34fae682e0e3"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>issuer_department</th>\n","      <th>issue_date</th>\n","      <th>title</th>\n","      <th>location</th>\n","      <th>document_id</th>\n","      <th>issuer</th>\n","      <th>document_type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Quốc Hội</td>\n","      <td>17/06/2020</td>\n","      <td>Luật Doanh Nghiệp Căn Cứ</td>\n","      <td>Hà Nội</td>\n","      <td>59/2020/QH14</td>\n","      <td>Nguyễn Thị Kim Ngân</td>\n","      <td>Luật</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Quốc Hội</td>\n","      <td>17/06/2025</td>\n","      <td>Luật Sửa Đổi , Bổ</td>\n","      <td>Hà Nội</td>\n","      <td>76/2025/QH15</td>\n","      <td>Trần Thanh Mẫn</td>\n","      <td>Luật</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Quốc Hội</td>\n","      <td>26/11/2024</td>\n","      <td>Luật Thuế Giá Trị Gia Tăng</td>\n","      <td>Hà Nội</td>\n","      <td>48/2024/QH15</td>\n","      <td>Trần Thanh Mẫn</td>\n","      <td>Luật</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Quốc Hội</td>\n","      <td>06/04/2016</td>\n","      <td>Luật Sửa Đổi , Bổ Sung Một Số Điều</td>\n","      <td>Hà Nội</td>\n","      <td>106/2016/QH13</td>\n","      <td>Nguyễn Thị Kim Ngân</td>\n","      <td>Luật</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Quốc Hội</td>\n","      <td>14/06/2025</td>\n","      <td>Luật Thuế Thu Nhập Doanh</td>\n","      <td>Hà Nội</td>\n","      <td>67/2025/QH15</td>\n","      <td>Trần Thanh Mẫn</td>\n","      <td>Luật</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Quốc Hội</td>\n","      <td>14/06/2025</td>\n","      <td>Luật Thuế Tiêu Thụ Đặc Biệt</td>\n","      <td>Hà Nội</td>\n","      <td>66/2025/QH15</td>\n","      <td>Trần Thanh Mẫn</td>\n","      <td>Luật</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Quốc Hội</td>\n","      <td>06/04/2016</td>\n","      <td>Luật Thuế Xuất Khẩu , Thuế Nhập Khẩu</td>\n","      <td>Hà Nội</td>\n","      <td>107/2016/QH13</td>\n","      <td>Unknown</td>\n","      <td>Luật</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Chính Phủ</td>\n","      <td>03/10/2025</td>\n","      <td>Nghị Định Quy Định Chi Tiết Về Việc Thực Hiện ...</td>\n","      <td>Hà Nội</td>\n","      <td>256/2025/NĐ-CP</td>\n","      <td>Unknown</td>\n","      <td>Nghị Định</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Chính Phủ</td>\n","      <td>02/04/2025</td>\n","      <td>Nghị Định Gia Hạn Thời Hạn Nộp Thuế Giá Trị Gi...</td>\n","      <td>Hà Nội</td>\n","      <td>82/2025/NĐ-CP</td>\n","      <td>Unknown</td>\n","      <td>Nghị Định</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Chính Phủ</td>\n","      <td>01/07/2025</td>\n","      <td>Nghị Định Quy Định Chi Tiết Thi Hành Một Số Điều</td>\n","      <td>Hà Nội</td>\n","      <td>181/2025/NĐ-CP</td>\n","      <td>Unknown</td>\n","      <td>Nghị Định</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Chính Phủ</td>\n","      <td>26/09/2025</td>\n","      <td>Nghị Định Quy Định Về</td>\n","      <td>Hà Nội</td>\n","      <td>254/2025/NĐ-CP</td>\n","      <td>Văn Bản</td>\n","      <td>Nghị Định</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Chính Phủ</td>\n","      <td>08/07/2025</td>\n","      <td>Nghị Định Sửa Đổi , Bổ Sung Nghị Định</td>\n","      <td>Hà Nội</td>\n","      <td>199/2025/NĐ-CP</td>\n","      <td>Unknown</td>\n","      <td>Nghị Định</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Chính Phủ</td>\n","      <td>08/06/2024</td>\n","      <td>Nghị Quyết Về Dự Án Nghị Quyết Của Quốc Hội Về...</td>\n","      <td>Hà Nội</td>\n","      <td>83/NQ-CP</td>\n","      <td>Lê Minh Khái</td>\n","      <td>Nghị Quyết</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Ủy Ban Nhân Dân</td>\n","      <td>01/10/2025</td>\n","      <td>Quyết Định Ban Hành Quy</td>\n","      <td>Cà Mau</td>\n","      <td>034/2025/QĐ-UBND</td>\n","      <td>Unknown</td>\n","      <td>Quyết Định</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Bộ Khoa Học Và Công Nghệ</td>\n","      <td>02/10/2025</td>\n","      <td>Quyết Định Ban Hành Kế</td>\n","      <td>Hà Nội</td>\n","      <td>3004/QĐ-BKHCN</td>\n","      <td>Unknown</td>\n","      <td>Quyết Định</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Bộ Nội Vụ</td>\n","      <td>03/10/2025</td>\n","      <td>Quyết Định Về Việc Công</td>\n","      <td>Hà Nội</td>\n","      <td>1136/QĐ-BNV</td>\n","      <td>Unknown</td>\n","      <td>Quyết Định</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Bộ Khoa Học Và Công Nghệ</td>\n","      <td>01/10/2025</td>\n","      <td>Quyết Định Ban Hành Kế</td>\n","      <td>Hà Nội</td>\n","      <td>2985/QĐ-BKHCN</td>\n","      <td>Unknown</td>\n","      <td>Quyết Định</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Bộ Tài Chính</td>\n","      <td>03/10/2025</td>\n","      <td>Thông Tư Bãi Bỏ Thông</td>\n","      <td>Hà Nội</td>\n","      <td>92/2025/TT-BTC</td>\n","      <td>Unknown</td>\n","      <td>Thông Tư</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Bộ Tài Chính</td>\n","      <td>23/07/2024</td>\n","      <td>Thông Tư Bãi Bỏ Một</td>\n","      <td>Hà Nội</td>\n","      <td>52/2024/TT-BTC</td>\n","      <td>Cao Anh Tuấn</td>\n","      <td>Thông Tư</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Ngân Hàng Nhà Nước Việt Nam</td>\n","      <td>30/09/2025</td>\n","      <td>Thông Tư Quy Định Về</td>\n","      <td>Hà Nội</td>\n","      <td>31/2025/TT-NHNN</td>\n","      <td>Đoàn Thái Sơn</td>\n","      <td>Thông Tư</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>Bộ Quốc Phòng</td>\n","      <td>02/10/2025</td>\n","      <td>Thông Tư Quy Định Tặng</td>\n","      <td>Hà Nội</td>\n","      <td>109/2025/TT-BQP</td>\n","      <td>Unknown</td>\n","      <td>Thông Tư</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              issuer_department  issue_date  \\\n","0                      Quốc Hội  17/06/2020   \n","1                      Quốc Hội  17/06/2025   \n","2                      Quốc Hội  26/11/2024   \n","3                      Quốc Hội  06/04/2016   \n","4                      Quốc Hội  14/06/2025   \n","5                      Quốc Hội  14/06/2025   \n","6                      Quốc Hội  06/04/2016   \n","7                     Chính Phủ  03/10/2025   \n","8                     Chính Phủ  02/04/2025   \n","9                     Chính Phủ  01/07/2025   \n","10                    Chính Phủ  26/09/2025   \n","11                    Chính Phủ  08/07/2025   \n","12                    Chính Phủ  08/06/2024   \n","13              Ủy Ban Nhân Dân  01/10/2025   \n","14     Bộ Khoa Học Và Công Nghệ  02/10/2025   \n","15                    Bộ Nội Vụ  03/10/2025   \n","16     Bộ Khoa Học Và Công Nghệ  01/10/2025   \n","17                 Bộ Tài Chính  03/10/2025   \n","18                 Bộ Tài Chính  23/07/2024   \n","19  Ngân Hàng Nhà Nước Việt Nam  30/09/2025   \n","20                Bộ Quốc Phòng  02/10/2025   \n","\n","                                                title location  \\\n","0                            Luật Doanh Nghiệp Căn Cứ   Hà Nội   \n","1                                   Luật Sửa Đổi , Bổ   Hà Nội   \n","2                          Luật Thuế Giá Trị Gia Tăng   Hà Nội   \n","3                  Luật Sửa Đổi , Bổ Sung Một Số Điều   Hà Nội   \n","4                            Luật Thuế Thu Nhập Doanh   Hà Nội   \n","5                         Luật Thuế Tiêu Thụ Đặc Biệt   Hà Nội   \n","6                Luật Thuế Xuất Khẩu , Thuế Nhập Khẩu   Hà Nội   \n","7   Nghị Định Quy Định Chi Tiết Về Việc Thực Hiện ...   Hà Nội   \n","8   Nghị Định Gia Hạn Thời Hạn Nộp Thuế Giá Trị Gi...   Hà Nội   \n","9    Nghị Định Quy Định Chi Tiết Thi Hành Một Số Điều   Hà Nội   \n","10                              Nghị Định Quy Định Về   Hà Nội   \n","11              Nghị Định Sửa Đổi , Bổ Sung Nghị Định   Hà Nội   \n","12  Nghị Quyết Về Dự Án Nghị Quyết Của Quốc Hội Về...   Hà Nội   \n","13                            Quyết Định Ban Hành Quy   Cà Mau   \n","14                             Quyết Định Ban Hành Kế   Hà Nội   \n","15                            Quyết Định Về Việc Công   Hà Nội   \n","16                             Quyết Định Ban Hành Kế   Hà Nội   \n","17                              Thông Tư Bãi Bỏ Thông   Hà Nội   \n","18                                Thông Tư Bãi Bỏ Một   Hà Nội   \n","19                               Thông Tư Quy Định Về   Hà Nội   \n","20                             Thông Tư Quy Định Tặng   Hà Nội   \n","\n","         document_id               issuer document_type  \n","0       59/2020/QH14  Nguyễn Thị Kim Ngân          Luật  \n","1       76/2025/QH15       Trần Thanh Mẫn          Luật  \n","2       48/2024/QH15       Trần Thanh Mẫn          Luật  \n","3      106/2016/QH13  Nguyễn Thị Kim Ngân          Luật  \n","4       67/2025/QH15       Trần Thanh Mẫn          Luật  \n","5       66/2025/QH15       Trần Thanh Mẫn          Luật  \n","6      107/2016/QH13              Unknown          Luật  \n","7     256/2025/NĐ-CP              Unknown     Nghị Định  \n","8      82/2025/NĐ-CP              Unknown     Nghị Định  \n","9     181/2025/NĐ-CP              Unknown     Nghị Định  \n","10    254/2025/NĐ-CP              Văn Bản     Nghị Định  \n","11    199/2025/NĐ-CP              Unknown     Nghị Định  \n","12          83/NQ-CP         Lê Minh Khái    Nghị Quyết  \n","13  034/2025/QĐ-UBND              Unknown    Quyết Định  \n","14     3004/QĐ-BKHCN              Unknown    Quyết Định  \n","15       1136/QĐ-BNV              Unknown    Quyết Định  \n","16     2985/QĐ-BKHCN              Unknown    Quyết Định  \n","17    92/2025/TT-BTC              Unknown      Thông Tư  \n","18    52/2024/TT-BTC         Cao Anh Tuấn      Thông Tư  \n","19   31/2025/TT-NHNN        Đoàn Thái Sơn      Thông Tư  \n","20   109/2025/TT-BQP              Unknown      Thông Tư  "]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":null,"id":"48cef937","metadata":{"id":"48cef937"},"outputs":[],"source":["# try_text = get_text_from_s3('legaldocstorage/luat_thue_tndn_2025.pdf')\n","\n","df_meta = extract_document_metadata(try_text, model, token2idx, idx2label, device)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}